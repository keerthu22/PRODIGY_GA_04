# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uesJIcxcObs53dfu-Lw0Qpuv7c5hs5mb
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
import matplotlib.pyplot as plt

def normalize(image):
    image = tf.cast(image, tf.float32) / 127.5 - 1  # Normalize to [-1, 1]
    return image

def add_noise(image):
    noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.1)  # Reduced noise
    noisy_image = tf.clip_by_value(image + noise, -1.0, 1.0)
    return noisy_image, image

# Use only 10% of the training data for faster testing
(train_data, _), ds_info = tfds.load('cifar10', split=['train[:10%]', 'train[90%:]'],
                                     as_supervised=True, with_info=True)

train_dataset = train_data.map(lambda x, y: normalize(x))
train_dataset = train_dataset.map(add_noise)
train_dataset = train_dataset.shuffle(1000).batch(32)

def downsample(filters, size, apply_batchnorm=True):
    initializer = tf.random_normal_initializer(0., 0.02)
    result = tf.keras.Sequential()
    result.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',
                                      kernel_initializer=initializer, use_bias=False))
    if apply_batchnorm:
        result.add(tf.keras.layers.BatchNormalization())
    result.add(tf.keras.layers.LeakyReLU())
    return result

def upsample(filters, size, apply_dropout=False):
    initializer = tf.random_normal_initializer(0., 0.02)
    result = tf.keras.Sequential()
    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                               padding='same', kernel_initializer=initializer,
                                               use_bias=False))
    result.add(tf.keras.layers.BatchNormalization())
    if apply_dropout:
        result.add(tf.keras.layers.Dropout(0.5))
    result.add(tf.keras.layers.ReLU())
    return result

def Generator():
    inputs = tf.keras.layers.Input(shape=[32, 32, 3])
    down1 = downsample(64, 4, apply_batchnorm=False)(inputs)
    down2 = downsample(128, 4)(down1)

    up1 = upsample(64, 4)(down2)
    concat1 = tf.keras.layers.Concatenate()([up1, down1])

    last = tf.keras.layers.Conv2DTranspose(3, 4, strides=2, padding='same', activation='tanh')(concat1)
    return tf.keras.Model(inputs=inputs, outputs=last)

def Discriminator():
    initializer = tf.random_normal_initializer(0., 0.02)
    inp = tf.keras.layers.Input(shape=[32, 32, 3], name='input_image')
    tar = tf.keras.layers.Input(shape=[32, 32, 3], name='target_image')

    x = tf.keras.layers.concatenate([inp, tar])
    down1 = downsample(64, 4, False)(x)
    down2 = downsample(128, 4)(down1)
    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down2)
    conv = tf.keras.layers.Conv2D(256, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1)
    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)
    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)
    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)
    last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)

    return tf.keras.Model(inputs=[inp, tar], outputs=last)

loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def generator_loss(disc_generated_output, gen_output, target):
    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)
    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))
    return gan_loss + (100 * l1_loss)

def discriminator_loss(disc_real_output, disc_generated_output):
    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)
    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)
    return real_loss + generated_loss

generator = Generator()
discriminator = Discriminator()

generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

# Warm-up models
dummy_input = tf.random.normal([1, 32, 32, 3])
_ = generator(dummy_input)
_ = discriminator([dummy_input, dummy_input])

@tf.function
def train_step(input_image, target):
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        gen_output = generator(input_image, training=True)

        disc_real_output = discriminator([input_image, target], training=True)
        disc_generated_output = discriminator([input_image, gen_output], training=True)

        gen_loss = generator_loss(disc_generated_output, gen_output, target)
        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)

    generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)
    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))

    return gen_loss, disc_loss

def generate_images(model, test_input, target):
    prediction = model(test_input, training=False)
    plt.figure(figsize=(10, 3))
    display_list = [test_input[0], target[0], prediction[0]]
    title = ['Noisy Input', 'Target (Clean)', 'Predicted (Denoised)']

    for i in range(3):
        plt.subplot(1, 3, i+1)
        plt.title(title[i])
        plt.imshow((display_list[i] + 1) / 2)
        plt.axis('off')
    plt.show()

EPOCHS = 100
for epoch in range(EPOCHS):
    print(f"Epoch {epoch+1}/{EPOCHS}")
    gen_loss_epoch = 0
    disc_loss_epoch = 0
    steps = 0
    for input_image, target in train_dataset:
        gen_loss, disc_loss = train_step(input_image, target)
        gen_loss_epoch += gen_loss
        disc_loss_epoch += disc_loss
        steps += 1

    print(f"Generator Loss: {gen_loss_epoch / steps:.4f}, Discriminator Loss: {disc_loss_epoch / steps:.4f}")

    # Only visualize every 10 epochs to reduce lag
    if (epoch + 1) % 10 == 0:
        sample_batch = next(iter(train_dataset))
        noisy_sample, clean_sample = sample_batch
        generate_images(generator, noisy_sample, clean_sample)